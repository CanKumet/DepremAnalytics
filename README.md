# ğŸŒ Kandilli Deprem Dashboard - GerÃ§ek ZamanlÄ± Deprem Ä°zleme Sistemi

Kandilli Rasathanesi verilerini kullanarak gerÃ§ek zamanlÄ± deprem takibi yapan, Kafka tabanlÄ± streaming mimarisiyle geliÅŸtirilmiÅŸ kapsamlÄ± analiz ve gÃ¶rselleÅŸtirme platformu.

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![Kafka](https://img.shields.io/badge/kafka-2.8+-orange.svg)
![Flask](https://img.shields.io/badge/flask-2.3+-green.svg)
![Spark](https://img.shields.io/badge/apache%20spark-3.4+-red.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

---

## ğŸ“‹ Proje Ã–zeti

Bu sistem, **Kandilli Rasathanesi**'nden anlÄ±k deprem verilerini Ã§ekerek, modern stream processing teknolojileri ile iÅŸleyen ve kullanÄ±cÄ± dostu bir web arayÃ¼zÃ¼ Ã¼zerinden sunan gerÃ§ek zamanlÄ± deprem izleme platformudur.

### ğŸ¯ Ana Hedef
TÃ¼rkiye'deki deprem aktivitelerini gerÃ§ek zamanlÄ± olarak takip etmek, analiz etmek ve gÃ¶rselleÅŸtirmek iÃ§in kapsamlÄ± bir veri pipeline sistemi oluÅŸturmak.

---

## ğŸš€ Ã–zellikler

- ğŸ”„ **GerÃ§ek ZamanlÄ± Veri Toplama**
  Kandilli Rasathanesi'nden her dakika gÃ¼ncel deprem verilerini otomatik olarak Ã§eker.

- ğŸ“¡ **Kafka Streaming Mimarisi**  
  YÃ¼ksek performanslÄ±, Ã¶lÃ§eklenebilir veri akÄ±ÅŸÄ± iÃ§in Apache Kafka kullanÄ±r.

- âš¡ **Apache Spark ile BÃ¼yÃ¼k Veri Ä°ÅŸleme**
  Streaming veriler Ã¼zerinde gerÃ§ek zamanlÄ± analiz, filtreleme ve aggregasyon iÅŸlemleri yapar.

- ğŸŒ **Flask Web Dashboard**
  WebSocket destekli, interaktif web arayÃ¼zÃ¼ ile anlÄ±k deprem bildirimleri.

- ğŸ“Š **Ä°statistik ve Analiz**
  - En aktif bÃ¶lgelerin tespiti
  - Ortalama bÃ¼yÃ¼klÃ¼k hesaplamalarÄ±  
  - GÃ¼nlÃ¼k/haftalÄ±k trend analizleri
  - Mâ‰¥4.0 depremlerde otomatik uyarÄ± sistemi

- ğŸ’¾ **SQLite Veri Depolama**
  TÃ¼m deprem verilerinin yerel veritabanÄ±nda gÃ¼venli saklanmasÄ±.

- ğŸ³ **Docker Container DesteÄŸi**
  Kolay kurulum ve deployment iÃ§in containerized mimari.

---

## ğŸ—ï¸ Sistem Mimarisi

```mermaid
graph TD
    A[Kandilli Rasathanesi] -->|HTTP Scraping| B[Python Producer]
    B -->|JSON Messages| C[Kafka Topic: deprem-verisi]
    C -->|Stream Processing| D[Apache Spark Streaming]
    C -->|Real-time Data| E[Flask Consumer]
    D -->|Analytics & Alerts| F[Console Output]
    E -->|WebSocket| G[Web Dashboard]
    E -->|Data Storage| H[SQLite Database]
    G -->|Live Updates| I[End Users]
    
    style A fill:#e1f5fe
    style C fill:#fff3e0
    style D fill:#f3e5f5
    style E fill:#e8f5e8
    style G fill:#fce4ec
```

---

## ğŸ› ï¸ Teknoloji Stack'i

| BileÅŸen | Teknoloji | Versiyon | AÃ§Ä±klama |
|---------|-----------|----------|----------|
| **Producer** | Python + Requests | 3.8+ | Kandilli verilerini Ã§eker ve Kafka'ya gÃ¶nderir |
| **Message Queue** | Apache Kafka | 2.8+ | YÃ¼ksek performanslÄ± veri akÄ±ÅŸÄ± |
| **Stream Processing** | Apache Spark | 3.4+ | GerÃ§ek zamanlÄ± veri analizi |
| **Web Framework** | Flask + SocketIO | 2.3+ | Web dashboard ve API servisleri |
| **Database** | SQLite | 3.x | Hafif, dosya tabanlÄ± veritabanÄ± |
| **Orchestration** | Docker Compose | 3.8 | Container yÃ¶netimi |
| **Frontend** | HTML5 + WebSocket | - | Responsive web arayÃ¼zÃ¼ |

---

## ğŸ“¦ Kurulum

### Ã–n Gereksinimler
- Python 3.8 veya Ã¼zeri
- Docker ve Docker Compose
- 4GB RAM (Spark iÃ§in Ã¶nerilen)
- 2GB disk alanÄ±

### 1. Projeyi Ä°ndirin
```bash
git clone https://github.com/your-username/kandilli-deprem-dashboard.git
cd kandilli-deprem-dashboard
```

### 2. Python Sanal OrtamÄ± OluÅŸturun
```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/Mac
source .venv/bin/activate
```

### 3. BaÄŸÄ±mlÄ±lÄ±klarÄ± Kurun
```bash
pip install -r requirements.txt
```

### 4. Kafka ve Zookeeper'Ä± BaÅŸlatÄ±n
```bash
docker-compose -f docker-compose-deprem.yml up -d
```

### 5. Sistemi BaÅŸlatÄ±n
```bash
python main.py
```

---

## ğŸ® KullanÄ±m

### Web Dashboard'a EriÅŸim
Sistem baÅŸlatÄ±ldÄ±ktan sonra:
- **Ana Dashboard**: http://localhost:5000
- **API Endpoints**:
  - Son depremler: `http://localhost:5000/api/recent/50`
  - Ä°statistikler: `http://localhost:5000/api/stats`

### Kafka UI (Opsiyonel)
Kafka cluster'Ä±nÄ± izlemek iÃ§in:
- **Kafka UI**: http://localhost:8180

---

## ğŸ“ Proje YapÄ±sÄ±

```
kandilli-deprem-dashboard/
â”œâ”€â”€ ğŸ“„ main.py                          # Ana orchestrator
â”œâ”€â”€ ğŸ“ producer/
â”‚   â””â”€â”€ ğŸ“„ kandilli_producer.py         # Veri toplama servisi
â”œâ”€â”€ ğŸ“ consumer/  
â”‚   â””â”€â”€ ğŸ“„ spark_streaming.py           # Spark streaming iÅŸlemleri
â”œâ”€â”€ ğŸ“ app/
â”‚   â”œâ”€â”€ ğŸ“„ app.py                       # Flask web uygulamasÄ±
â”‚   â””â”€â”€ ğŸ“ templates/
â”‚       â””â”€â”€ ğŸ“„ index.html               # Web dashboard
â”œâ”€â”€ ğŸ“„ docker-compose-deprem.yml        # Kafka infrastructure
â”œâ”€â”€ ğŸ“„ requirements.txt                 # Python dependencies
â””â”€â”€ ğŸ“„ README.md                        # Bu dosya
```

---

## ğŸ”§ KonfigÃ¼rasyon

### Kafka AyarlarÄ±
```python
# producer/kandilli_producer.py
KAFKA_BOOTSTRAP_SERVERS = 'localhost:9192'
KAFKA_TOPIC = 'deprem-verisi'
FETCH_INTERVAL = 60  # saniye
```

### Spark Streaming AyarlarÄ±
```python
# consumer/spark_streaming.py
SPARK_APP_NAME = "KandilliDepremAnalizi"
KAFKA_BOOTSTRAP_SERVERS = "localhost:9192"
ALERT_THRESHOLD = 4.0  # Minimum bÃ¼yÃ¼klÃ¼k uyarÄ±sÄ±
```

### Flask AyarlarÄ±
```python
# app/app.py
SECRET_KEY = 'kandilli-deprem-dashboard-2024'
HOST = '0.0.0.0'
PORT = 5000
DEBUG = True
```

---

## ğŸ“Š Veri ÅemasÄ±

### Deprem Veri Modeli
```json
{
  "tarih": "2024.01.15",
  "saat": "14:23:45",
  "enlem": 40.1234,
  "boylam": 29.5678,
  "derinlik_km": 8.2,
  "buyukluk": 3.4,
  "yer": "MARMARA DENÄ°ZÄ°"
}
```

### VeritabanÄ± ÅemasÄ±
```sql
CREATE TABLE depremler (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    tarih TEXT,
    saat TEXT,
    enlem REAL,
    boylam REAL,
    derinlik_km REAL,
    buyukluk REAL,
    yer TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

---

## ğŸš¨ UyarÄ± Sistemi

Sistem, belirlenen kriterlere gÃ¶re otomatik uyarÄ±lar Ã¼retir:

| BÃ¼yÃ¼klÃ¼k | UyarÄ± Seviyesi | Aksiyon |
|----------|---------------|---------|
| M â‰¥ 4.0 | ğŸŸ¡ Orta | Console log + WebSocket bildirimi |
| M â‰¥ 5.0 | ğŸŸ  YÃ¼ksek | VurgulanmÄ±ÅŸ bildirim |
| M â‰¥ 6.0 | ğŸ”´ Kritik | Acil durum bildirimi |

---

## ğŸ“ˆ Performans Metrikleri

- **Veri Toplama SÄ±klÄ±ÄŸÄ±**: 60 saniye
- **Kafka Throughput**: ~1000 mesaj/saniye
- **Spark Micro-batch**: 5 saniye
- **WebSocket Latency**: <100ms
- **VeritabanÄ± Response**: <10ms

---

## ğŸ³ Docker Deployment

### GeliÅŸtirme OrtamÄ±
```bash
# Kafka cluster'Ä± baÅŸlat
docker-compose -f docker-compose-deprem.yml up -d

# UygulamayÄ± yerel olarak Ã§alÄ±ÅŸtÄ±r
python main.py
```

### Production OrtamÄ±
```bash
# TÃ¼m servisleri container'da Ã§alÄ±ÅŸtÄ±r
docker-compose up -d --scale spark-worker=3
```

---

## ğŸ§ª Test ve DoÄŸrulama

### Unit Testler
```bash
python -m pytest tests/ -v
```

### Sistem Testleri
```bash
# Producer testi
python producer/kandilli_producer.py --test

# Consumer testi  
python consumer/spark_streaming.py --validation-mode

# API testleri
curl http://localhost:5000/api/stats
```

---

## ğŸ” Ä°zleme ve Logging

### Log Seviyeleri
- **INFO**: Normal sistem operasyonlarÄ±
- **WARNING**: Beklenen ancak Ã¶nemli durumlar
- **ERROR**: Hata durumlarÄ±
- **CRITICAL**: Sistem kesintileri

### Monitoring Endpoints
- **Health Check**: `/health`
- **Metrics**: `/metrics`
- **Status**: GerÃ§ek zamanlÄ± sistem durumu

---

## ğŸ¤ KatkÄ±da Bulunma

1. Bu repository'yi fork edin
2. Feature branch oluÅŸturun (`git checkout -b feature/yeni-ozellik`)
3. DeÄŸiÅŸikliklerinizi commit edin (`git commit -am 'Yeni Ã¶zellik eklendi'`)
4. Branch'inizi push edin (`git push origin feature/yeni-ozellik`)
5. Pull Request oluÅŸturun

### GeliÅŸtirme KurallarÄ±
- PEP 8 coding standards'Ä±na uyun
- TÃ¼m fonksiyonlar iÃ§in docstring yazÄ±n
- Unit test coverage %80'in Ã¼zerinde olsun
- Type hints kullanÄ±n

---

## ğŸ“‹ TODO Listesi

- [ ] Redis cache entegrasyonu
- [ ] PostgreSQL desteÄŸi
- [ ] Harita gÃ¶rselleÅŸtirmesi (Leaflet.js)
- [ ] Mobile responsive tasarÄ±m
- [ ] Email/SMS bildirim sistemi
- [ ] Historical data analytics
- [ ] Machine learning prediction models
- [ ] API rate limiting
- [ ] Kubernetes deployment manifests

---

## âš–ï¸ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in [LICENSE](LICENSE) dosyasÄ±na bakÄ±n.

---

## ğŸ™ TeÅŸekkÃ¼rler

- [Kandilli Rasathanesi](http://www.koeri.boun.edu.tr/) - Deprem verileri saÄŸladÄ±ÄŸÄ± iÃ§in
- [Apache Kafka](https://kafka.apache.org/) - Streaming infrastructure
- [Apache Spark](https://spark.apache.org/) - Big data processing
- [Flask](https://flask.palletsprojects.com/) - Web framework

---

## ğŸ“ Ä°letiÅŸim

**GeliÅŸtirici**: [GitHub KullanÄ±cÄ± AdÄ±nÄ±z](https://github.com/your-username)  
**E-posta**: your-email@domain.com  
**Proje URL**: https://github.com/your-username/kandilli-deprem-dashboard

---

## ğŸ“ˆ Proje Ä°statistikleri

![GitHub stars](https://img.shields.io/github/stars/your-username/kandilli-deprem-dashboard?style=social)
![GitHub forks](https://img.shields.io/github/forks/your-username/kandilli-deprem-dashboard?style=social)
![GitHub issues](https://img.shields.io/github/issues/your-username/kandilli-deprem-dashboard)
![GitHub last commit](https://img.shields.io/github/last-commit/your-username/kandilli-deprem-dashboard)

---

<div align="center">

**â­ Bu projeyi beÄŸendiyseniz star vermeyi unutmayÄ±n! â­**

*Son gÃ¼ncelleme: 2024*

</div>
